[ { "title": "Motion Planning Augmented Reinforcement Learning for Long-Horizon Visual Robot Manipulation", "url": "/projects/mprl/", "categories": "DRL", "tags": "DRL, Motion Planner, Long-horizon Task, Skill Chaining, Vision-Language Model, Object Detection, Habitat, Mobile Manipulator", "date": "2025-06-24 00:00:00 +0900", "snippet": "Overview Motion Planning Augmented Reinforcement Learning for Long-Horizon Visual Robot Manipulation was a personal project I worked on in Helper Lab, Sungkyunkwan university supervised by professor Mun-Taek Choi. This research project is based on my Master’s thesis in Intelligent Robotics. Goal The objective of this study is to enhance the learning efficiency of Reinforcement Learning (RL) by incorporating physics-informed guidance, targeting a mobile manipulator in a simulated household environment.Description The methodology builds upon the framework proposed by Gu et al. in “Multi-Skill Mobile Manipulation for Object Rearrangement” (Paper Link). To address sparse rewards and inefficient exploration in RL, motion planning is incorporated into the reward function as guidance. The task is decomposed into pick, place, and navigation sub-skills, which are executed sequentially through point-based skill chaining. Vision-Language Model (VLM) augmented object detection is implemented by combining YOLOv7 with BLIP-2, enabling open-vocabulary semantic recognition. The system is developed using the Habitat Simulator. The robot setup is based on the Fetch Robot, consisting of a differential-drive mobile base and a 7-DOF arm equipped with a parallel-jaw gripper. RGB-D cameras are mounted on both the head and the arm. Note that abstract grasping is used, where physical contact dynamics are not modeled, and grasps are considered successful when within a specified positional threshold. As a result, each sub-skill demonstrated faster convergence in success rate compared to the baseline distance-based methods without motion planning augmentation. Furthermore, the cumulative success rate for long-horizon tasks reached a relatively high value of 0.72. References Youtube Link 1 Youtube Link 2 Paper Link " }, { "title": "Deep Temporal Clustering for Gait Pattern", "url": "/projects/dtcgp/", "categories": "Rehabilitation", "tags": "Rehabilitation, Post-stroke, Hemiplegia, Gait Patterns, Time-series Data, Deep Clustering", "date": "2025-01-11 00:00:00 +0900", "snippet": "Overview Deep Temporal Clustering for Gait Pattern (DTCGP) was a collaborative project I worked in Helper Lab, Sungkyunkwan university supervised by professor Mun-Taek Choi. The paper was accepted and published by MDPI Bioengineering. Goal The objective of this cross-sectional study aims to implement an end-to-end deep learning (DL) approach that directly utilizes time-series gait cycle data as model input, eliminating the need for manual feature extraction.Description DTCGP uses the data consisted of time-normalized joint angle trajectories, motion-captured during independent gaits of sub-acute hemiplegic post-stroke patients at Samsung Medical Center (SMC). As input data for the DL, joint angles and angular velocity trajectories in the sagittal plane are utilized as an instance. DTCGP is based on the existing Deep Temporal Clustering (DTC) algorithm, which is composed of temporal autoencoder and clustering layer with simultaneous optimization, by Madiraju et al. With hyperparameter tuning tailored for kinematic gait cycle data, six optimal clusters are selected with a silhouette score of 0.2831. To clarify the characteristics of the selected groups, in-depth statistics of spatiotemporal, kinematic, and clinical features are presented. References Paper Link" }, { "title": "Rotary Pendulum with PPO and Domain Randomization", "url": "/projects/rotary-pen-drl/", "categories": "DRL", "tags": "DRL, Control, Mujoco, Gym, Pytorch, PPO, Domain Randomization", "date": "2024-06-20 00:00:00 +0900", "snippet": "Overview Rotary Pendulum with PPO and Domain Randomization is a personal project.Goal To achieve robust control of a rotary (Furuta) pendulum using PPO and Domain Randomization (DR) in simulation, in preparation for real-world implementation where physical inaccuracies may occur.Description The source code is written in Python language and the modeling of rotary pendulum in XML format is from macstepien’s furuta_pendulum repository. The project utilizes PPO algorithm from the Stable-Baselines3 library as DRL framework. Simulations are conducted using Mujoco. Domain Randomization (DR) is a method that considers the real-world environment as one of many possible random variations, enabling the simulation to learn under diverse physical conditions. Three physical properties are randomized: the mass of the pendulum, the length of the pendulum, and the mass of the rod (arm1) connecting the pendulum to the central cylinder. The state space includes the angles and angular velocities of arm1 and the pendulum. The action space consists of actuator torques ranging from -0.4 to 0.4. The reward function is designed to encourage the pendulum to maintain the upright position. References rotary_pendulum_ppo [Github Link]" }, { "title": "Dynamic Robot Planner with Contextual Awareness via LLM", "url": "/projects/dynacon/", "categories": "Navigation", "tags": "Navigation, LLM, Prompt Engineering, ROS1", "date": "2023-09-27 00:00:00 +0900", "snippet": "Overview Dynamic Robot Planner with Contextual Awareness via LLM (DynaCon) was a collaborative project I worked with TaeHyeon Kim, Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and supervised by professor Donghan Kim and Byung-Cheol Min. The paper was submitted to the IEEE International Conference on Robotics and Automation (ICRA 2024). Goal To provide mobile robots with contextual awareness and dynamic adaptability during navigation, eliminating the reliance of traditional maps.Description DynaCon consists of real-time feedback, prompt engineering, and navigation task. In the real-time feedback section, the Object Server continuously provides an information about nearby objects and the current position of the robot to update the object list. In the prompt engineering phase, a uniquely structured prompt is sent to the Large Language Model (LLM) to output the desired object, serving as the main task of navigation. For navigation task, ROS1 move_base package is applied. References Paper Link Website Supplementary Video " }, { "title": "Mobile Robot Platform with ToF Sensors", "url": "/projects/mobile-platform/", "categories": "Navigation", "tags": "Navigation, Mobile Robot, ToF, Lidar, ROS1, Autodesk Inventor", "date": "2022-12-28 00:00:00 +0900", "snippet": "Overview This was a collaborative project I worked with Seung Jin Yang under the guidance of the Robotics Innovatory lab. It was submitted to AI-ICT Creative Idea Contest hosted by Sungkyunkwan University. - 3rd prize Goal To design a scalable mobile platform hardware that can support additional sensors, algorithms, and hardware for future use. Also, to enable basic navigation using a LiDAR sensor. Description Hardware Designed with reference to various commercial mobile robots, it maintains a low height to allow for additional modules on top and adopts a rounded shape to account for environments with many people nearby. Using aluminum profiles for the internal structure to enhance robot stability. For cost efficiency, the robot’s exterior was designed in AutoDesk Inventor and then 3D printed for assembly. Navigation Utilizes a YDLidar sensor to gather surrounding environmental data, enabling distance measurements essential for navigation tasks. Operates in ROS1 Noetic environment. Mapping: Hector mapping Localization: AMCL Global navigation: A* algorithm Local navigation: Dynamic Window Approach (DWA) " }, { "title": "Vision-based Track Following Vehicle", "url": "/projects/vision-track/", "categories": "Vision", "tags": "Vision, Navigation, Mobile platform, Jetson Nano, Object Detection, YOLOv3, ResNet", "date": "2022-12-09 00:00:00 +0900", "snippet": "Overview This was a collaborative project I worked with Donghyuk Park, Yunseong Jeon, and Hyeonggeun Hong. The project was submitted to Autonomous Driving Capstone Design Contest hosted by Sungkyunkwan University. - 1st prize This contest was linked with the Autonomous Driving Capstone Design (ICE3051-41) course at Sungkyunkwan University, supervised by Professors Jonghoek Kim, Eunbyung Park, and Il Yong Chun. Goal To create a car-like model, combining a child’s electric car with a Jetson Nano board, that can drive along a track bordered by red traffic cones and stop upon recognizing a stop sign at the end of the road.Description Car-like model design A Jetson Nano platform with monocular camera was mounted onto a commercially available child’s car to process data and provide control inputs to the car’s motor. Track Following Red traffic cones line both sides of the road, defining its boundaries, and the goal is for the car to follow the track without colliding with or veering away from these cones. Using deep learning, the steering angle was trained based on specific track conditions. Input data was a 224x224 image array from the camera, and the model used ResNet18. To improve training efficiency, preprocessing was applied to the image array before feeding it into the model; the non-road upper portion of the image was cropped, and only the red color of the cones was tracked. Object Detection At the end of the road, the stop sign is detected, triggering a signal to bring the car’s speed to zero. Deep learning was used to train the model for stop sign detection. The input data was a 224x224 image array from the camera, and the model utilized Yolov5s. " }, { "title": "Object Tracker", "url": "/projects/object-tracker/", "categories": "Vision", "tags": "Vision, Object Detection, Depth Camera, ROS1, YOLOv3, darknet", "date": "2022-01-09 00:00:00 +0900", "snippet": "Overview Object Tracker is a personal project I worked with TaeHyeon Kim.Goal To simultaneously detect various objects and track their location with different distances.Description The source code is written in Python language and the package is built by CMake. This package enables to recognize objects, publish TF topic, anddisplay distances for each recognized object using Intel RealSense depth camera (D435i) under Linux Ubuntu OS and ROS1. This package utilizes the Intel RealSense SDK, the RealSense ROS package, and the Darknet ROS package. Using the RealSense-related packages, it processes RGB images and point cloud data from the depth camera. With the Darknet ROS package, it outputs bounding boxes and recognition labels as ROS topics by implementing YOLOv3 on both GPU and CPU. References object_tracker [Github Link]" } ]
